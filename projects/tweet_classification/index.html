<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tweet classification | Mohan Ramesh</title> <meta name="author" content="Mohan Ramesh"/> <meta name="description" content="a basic algorithm that detects disasters from twitter feeds"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://zybermonk.github.io/projects/tweet_classification/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohan </span>Ramesh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blogs</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tweet classification</h1> <p class="post-description">a basic algorithm that detects disasters from twitter feeds</p> </header> <article> <p><strong>TWEET CLASSIFICATION: DISASTER DETECTION</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="s">'train.csv'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">train_df</span><span class="p">.</span><span class="nf">head</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">train_df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">train_df</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">())</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   id keyword geo_loc                                          text_body  \
0   1     NaN     NaN  Our Deeds are the Reason of this #earthquake M...   
1   4     NaN     NaN             Forest fire near La Ronge Sask. Canada   
2   5     NaN     NaN  All residents asked to 'shelter in place' are ...   
3   6     NaN     NaN  13,000 people receive #wildfires evacuation or...   
4   7     NaN     NaN  Just got sent this photo from Ruby #Alaska as ...   

   label  
0      1  
1      1  
2      1  
3      1  
4      1  
(7613, 5)
id              0
keyword        61
geo_loc      2533
text_body       0
label           0
dtype: int64
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#checking the label ratio
</span><span class="n">train_df</span><span class="p">[</span><span class="s">'label'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0    4342
1    3271
Name: label, dtype: int64
</code></pre></div></div> <blockquote> <p><strong>EDA</strong></p> </blockquote> <p>Looks like the data has 3 independent features (<strong>keyword,geo_loc,text_body</strong>) that determine the <strong>label</strong> (dependent feature). However, the <em>geo_loc</em> variable has about one-third of the values as NaN. It can still be used as 66% is considerably sufficient amount of data. But manually examining the data reveals the feature doesn’t really provide useful information in terms of geographical location or any significant inidicators that contribute in determining the output variable. Whereas it contains text information expressed with complete use of the individual’s ‘freedom of speech’. An advance algorithm that can extract useful semantics and express the variable’s true intention will be best to handle this type of feature, hence it is skipped in this project. <br> So, there are now 2 variables determining the output;</p> <ol> <li> <strong>keyword</strong>: categorical</li> <li> <strong>text_body</strong>: text</li> </ol> <p>Using these 2 variables a baseline model to classify the tweets containing disaster intel or not will be created.</p> <blockquote> <p><strong>Pre Processing and Feature Engineering</strong></p> </blockquote> <p>First, the text feature is handled for both train and test datasets</p> <ul> <li>TRAIN</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>
<span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="n">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="c1">#Basic preprocessing
</span><span class="n">l</span><span class="o">=</span><span class="nc">WordNetLemmatizer</span><span class="p">()</span>

<span class="n">core</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">)):</span>
    <span class="n">r</span><span class="o">=</span><span class="p">[]</span>
    <span class="n">r</span><span class="o">=</span><span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="s">'[^a-zA-Z0-9]'</span><span class="p">,</span> <span class="s">' '</span><span class="p">,</span><span class="n">train_df</span><span class="p">[</span><span class="s">'text_body'</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
    <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>
    <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
    <span class="n">r</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">.</span><span class="nf">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">r</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">))]</span>
    <span class="n">r</span><span class="o">=</span><span class="s">' '</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="n">core</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

</code></pre></div></div> <ul> <li>TEST</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_df</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="s">'test.csv'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">test_df</span><span class="p">.</span><span class="nf">head</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">test_df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">test_df</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">())</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   id keyword geo_loc                                          text_body
0   0     NaN     NaN                 Just happened a terrible car crash
1   2     NaN     NaN  Heard about #earthquake is different cities, s...
2   3     NaN     NaN  there is a forest fire at spot pond, geese are...
3   9     NaN     NaN           Apocalypse lighting. #Spokane #wildfires
4  11     NaN     NaN      Typhoon Soudelor kills 28 in China and Taiwan
(3263, 4)
id              0
keyword        26
geo_loc      1105
text_body       0
dtype: int64
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Basic preprocessing
</span><span class="n">l1</span><span class="o">=</span><span class="nc">WordNetLemmatizer</span><span class="p">()</span>
<span class="n">core1</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">)):</span>
    <span class="n">r1</span><span class="o">=</span><span class="p">[]</span>
    <span class="n">r1</span><span class="o">=</span><span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="s">'[^a-zA-Z0-9().]'</span><span class="p">,</span> <span class="s">' '</span><span class="p">,</span><span class="n">test_df</span><span class="p">[</span><span class="s">'text_body'</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
    <span class="n">r1</span><span class="o">=</span><span class="n">r1</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>
    <span class="n">r1</span><span class="o">=</span><span class="n">r1</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
    <span class="n">r1</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">.</span><span class="nf">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">r1</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">))]</span>
    <span class="n">r1</span><span class="o">=</span><span class="s">' '</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
    <span class="n">core1</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
</code></pre></div></div> <p>Now, the Categorical feature is handled for both the train and test datasets</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="s">'The number of unique keywords in training data is: '</span> <span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s">'keyword'</span><span class="p">].</span><span class="nf">unique</span><span class="p">()))</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'The number of unique keywords in test data is: '</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="s">'keyword'</span><span class="p">].</span><span class="nf">unique</span><span class="p">()))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The number of unique keywords in training data is:  222
The number of unique keywords in test data is:  222
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span><span class="p">[</span><span class="s">'keyword'</span><span class="p">]</span><span class="o">=</span><span class="n">train_df</span><span class="p">[</span><span class="s">'keyword'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="s">'no_key'</span><span class="p">)</span>
<span class="n">test_df</span><span class="p">[</span><span class="s">'keyword'</span><span class="p">]</span><span class="o">=</span><span class="n">test_df</span><span class="p">[</span><span class="s">'keyword'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="s">'no_key'</span><span class="p">)</span>
</code></pre></div></div> <p>The missing data is filled with a custom categorical label, now the different categories will be converted to numerical labels</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#one hot encoding
</span><span class="n">keys</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">get_dummies</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s">'keyword'</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">keys2</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">get_dummies</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="s">'keyword'</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">keys</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">keys2</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(7613, 221)
(3263, 221)
</code></pre></div></div> <p>There are 221 labels representing 222 keywords (as the final keyword will be the absense of other keys).</p> <blockquote> <p><strong>Vectorization</strong></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">vr</span><span class="o">=</span><span class="nc">TfidfVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span><span class="n">stop_words</span><span class="o">=</span><span class="s">"english"</span><span class="p">)</span>
<span class="n">vec</span><span class="o">=</span><span class="n">vr</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">core</span><span class="p">)</span> <span class="c1">#training vectors
</span><span class="n">vec1</span><span class="o">=</span><span class="n">vr</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">core1</span><span class="p">)</span> <span class="c1">#test vectors, used to generate final output
</span></code></pre></div></div> <p>Since the dimensions are huge, about 20kplus individual features, a dimensionality reduction method is used to try and reduce redundant features. Again intuition based max feature selection of 5000.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xtrain</span><span class="o">=</span><span class="n">vec</span><span class="p">.</span><span class="nf">toarray</span><span class="p">()</span>
<span class="c1">#print(xtrain.shape)
</span><span class="n">xtrain_df</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">xtrain_df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(7613, 5000)
</code></pre></div></div> <p>Now, a new combined dataframe contating the categorical encoded values and the vectors is created for the training purpose</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># all the independent variables in this case
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">keys</span><span class="p">,</span><span class="n">xtrain_df</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span><span class="p">.</span><span class="nf">info</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 7613 entries, 0 to 7612
Columns: 5221 entries, accident to 4999
dtypes: float64(5000), uint8(221)
memory usage: 292.0 MB
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># target varible creation
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s">'label'</span><span class="p">].</span><span class="n">values</span>

<span class="c1">#cross validation
</span><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">xt</span><span class="p">,</span><span class="n">xte</span><span class="p">,</span><span class="n">yt</span><span class="p">,</span><span class="n">yte</span><span class="o">=</span><span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <blockquote> <p><strong>Model1</strong></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="n">model_detect</span><span class="o">=</span><span class="nc">GaussianNB</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yp</span><span class="o">=</span><span class="n">model_detect</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">xte</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">yp</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 1 1 ... 1 0 1]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span><span class="n">yp</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[497, 389],
       [146, 491]], dtype=int64)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="nf">accuracy_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span><span class="n">yp</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.6487196323046619
</code></pre></div></div> <p>A good 64.8% accuracy is shown on the cross validation test set, However and important observation was noticed when all the 20k features were used to train the Gaussian naive bayes model, the model accuracy was significanlty low, about 61%. And when the features were further reduced by using <em>min_df</em> and <em>max_df</em> parameters, the accuracy went upto 75%. This suggests that, as the features increase the gaussian naive bayes model loses its performance!</p> <blockquote> <p><strong>Model2</strong></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="n">model2_detect</span><span class="o">=</span><span class="nc">MultinomialNB</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yp2</span><span class="o">=</span><span class="n">model2_detect</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">xte</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">yp2</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0 0 0 ... 1 0 1]
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span><span class="n">yp2</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[747, 139],
       [188, 449]], dtype=int64)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span><span class="n">yp2</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.7852921864740644
</code></pre></div></div> <p>However, when Multinomial naive bayes model is used the accuracy was consistant for both 20k features and when feautres were reduced by almost 85%. This shows that the Multinomial naive bayes model is somewhat efficient in terms of the resources used as well the performance given</p> <blockquote> <p><strong>Intuition specific to the current problem: Disaster classification</strong></p> </blockquote> <p>In the case of classifying disasters, if a normal event is classified as a disaster that wouldn’t much of a problem. However, if an actual disaster is classified as a regular event, that is an actual disaster XD. So, the metric we need to focus here is the <strong>‘False negative’</strong>. Hence, we have to evaluate the “Recall” of the model to validate the efficiency.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#GaussianNB
</span><span class="nf">recall_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span><span class="n">yp</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.7708006279434851
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#MultinomialNB
</span><span class="nf">recall_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span><span class="n">yp2</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.7048665620094191
</code></pre></div></div> <p>!! The recall for the GaussianNB model is better, meaning it is predicting lesser false negatives. This might also be due to the presence of lesser negative classes during the cross validation too. However the accuracy of MultinomialNB is far higher than the GaussianNB for any number of features. And also, both the models showed a similar recall when further hyperparameters are tuned. So, the model using least resources to produce effective results can be used according to the specific use case.</p> <blockquote> <p><strong>Other Models</strong></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">cl</span><span class="o">=</span><span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"rbf"</span><span class="p">)</span>
<span class="n">model3_detect</span><span class="o">=</span><span class="n">cl</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">yt</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yp3</span><span class="o">=</span><span class="n">model3_detect</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">xte</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span><span class="n">yp3</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.7839789888378201
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">recall_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span><span class="n">yp3</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.6671899529042387
</code></pre></div></div> <p>The SVM model seems to be performing worse than the previous models, however the choice of kernel and other parameter tuning might give us a different result. When used a linear kernel, The accuracy came out to be 80% and the Recall was about 69% which wasn’t too far off from the previous models, but also with a better accuracy. Hence, for the actual prediction a generalized SVM with a linear kernal can be used in this case.</p> <blockquote> <p><strong>Final Result</strong></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cl2</span><span class="o">=</span><span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">"linear"</span><span class="p">)</span>
<span class="n">disaster_classifier</span><span class="o">=</span><span class="n">cl2</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xtest</span><span class="o">=</span><span class="n">vec1</span><span class="p">.</span><span class="nf">toarray</span><span class="p">()</span>
<span class="n">xtest_df</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">xtest_df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(3263, 5000)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X2</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">keys2</span><span class="p">,</span><span class="n">xtest_df</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X2</span><span class="p">.</span><span class="nf">info</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 3263 entries, 0 to 3262
Columns: 5221 entries, accident to 4999
dtypes: float64(5000), uint8(221)
memory usage: 125.2 MB
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Result</span><span class="o">=</span><span class="n">disaster_classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_df</span><span class="p">[</span><span class="s">'disaster_alert'</span><span class="p">]</span><span class="o">=</span><span class="n">Result</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>id</th> <th>keyword</th> <th>geo_loc</th> <th>text_body</th> <th>disaster_alert</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>0</td> <td>no_key</td> <td>NaN</td> <td>Just happened a terrible car crash</td> <td>0</td> </tr> <tr> <th>1</th> <td>2</td> <td>no_key</td> <td>NaN</td> <td>Heard about #earthquake is different cities, s...</td> <td>1</td> </tr> <tr> <th>2</th> <td>3</td> <td>no_key</td> <td>NaN</td> <td>there is a forest fire at spot pond, geese are...</td> <td>0</td> </tr> <tr> <th>3</th> <td>9</td> <td>no_key</td> <td>NaN</td> <td>Apocalypse lighting. #Spokane #wildfires</td> <td>1</td> </tr> <tr> <th>4</th> <td>11</td> <td>no_key</td> <td>NaN</td> <td>Typhoon Soudelor kills 28 in China and Taiwan</td> <td>0</td> </tr> </tbody> </table> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_df</span><span class="p">[</span><span class="s">'disaster_alert'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0    2363
1     900
Name: disaster_alert, dtype: int64
</code></pre></div></div> <p>The training data had about 42.9% of the tweets classified as containing info on disasters, and the test data resulted in about 27.5% of the tweets to be about disasters. Which might be closer to the actual true information as the sample size of the test data was about half of the train data, and also the linear SVM model has an accuracy about 80%. Hence, for further improvements, more feature engineering and hyperparameter tuning will be significantly helpful.</p> <blockquote> <p><strong>Conclusion</strong></p> </blockquote> <p>With models based on decision trees (XGboost,Randomforst etc) the results would be much better. With these boosted techniques along with neural networks and ensemble models, the classification can be performed in an enterprise level and can even be productized. Fundamentally, better extraction techniques, feature engineering, and a solid hyperparameter tuning will mostly does the job of classification. This exercise only focuses on developing a baseline model with a decent classification ability.<br> An ideal way to solve this kind of problem will be something like this:</p> <div> <figure> <picture> <img src="/assets/img/projects/tweet/ss3.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Mohan Ramesh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>