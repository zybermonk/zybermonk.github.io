<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The simulation hypothesis | Mohan Ramesh</title> <meta name="author" content="Mohan Ramesh"/> <meta name="description" content="Predecessor events about a few centuries before it…"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://zybermonk.github.io/blog/2021/tsh/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "The simulation hypothesis",
      "description": "Predecessor events about a few centuries before it…",
      "written on": "March 25, 2021",
      "authors": [
        {
          "author": "Mohan Ramesh",
          "authorURL": "https://github.com/Neuromancer24",
          "affiliations": [
            {
              "name": "None",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohan </span>Ramesh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>The simulation hypothesis</h1> <p>Predecessor events about a few centuries before it…</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#papers-in-the-field-and-data-sources">Papers in the field and Data Sources</a></div> <div><a href="#model-and-variables-proposed">Model and Variables proposed</a></div> <div><a href="#a-priori-expectation">A priori expectation</a></div> </nav> </d-contents> <h1 id="introduction">Introduction</h1> <p>End is near, is it? We have been hearing this over the past couple of decades. Is it just an irrational fideism or maybe the prophecies have an error of -10+ , -100+ and so on. Schmidhuber try to extend the naive analysis of past computer science breakthroughs in the intro-duction, which predicts that computer history will converge in an Omega point or historic singularity X around 2040. Could it be that such lists just reflect the human way of allocating memory space to past events Maybe there is a general rule for both the individual memory of single humans and the collective memory of entire societies and their history books. He basically tries to say if history repeats itself, which is the first research which I try to answer through my project.</p> <p>1) <a href="https://people.idsia.ch/~juergen/history.html" target="_blank" rel="noopener noreferrer">Will History Converge?</a> Is it a co-incidence or if there is a pattern repeating itself? This research question will be answered based on the research question 2.</p> <p>One of the breakthroughs that can cause this next big revolution is the creation of a super Intelligence, or more specifically, Artificial General Intelligence. According to the above convergence hypothesis, we are due to see this AGI in the next few decades. This idea gives rise to my second research question, that is,</p> <p>2) What is the possibility of such an AGI to come into existence, if Yes, what is probability of an intelligence explosion?</p> <p>[ There are two possible types of intelligence speedup: one due to faster operation of an intelligent system (clock speed increase or the hardware) and one due to an improvement in the type of mechanisms that implement the thought processes (‘‘depth of thought’’ increase or the software). Obviously, both could occur at once (and there may be significant synergies), but the latter is ostensibly more difficult to achieve, and may be subject to fundamental limits that we do not understand. Speeding up the hardware, on the other hand, is something that has been going on for a long time and is more mundane and reliable. Notice that both routes lead to greater ‘‘intelligence,’’ because even a human level of thinking and creativity would be more effective if it were happening a thousand times faster than it does now. It seems possible that the general class of AGI systems can be architected to take better advantage of improved hardware than would be the case with intelligent systems very narrowly imitative of the human brain. But even if this is not the case, brute hardware speedup can still yield dramatic intelligent improvement.] This section of text from <a href="https://lists.extropy.org/pipermail/extropy-chat/2011-January/063255.html" target="_blank" rel="noopener noreferrer">Richard Loosemore</a>, suggests that the explosion can be measured and we can clearly discard limiting factors such as investment ability, software complexity and hardware advancement.</p> <p><strong>Hypothesis :</strong> Once an AI system with roughly human-level general intelligence is created, an ‘‘intelligence explosion’’ involving the relatively rapid creation of increasingly more generally intelligent AI systems will very likely ensue, resulting in the rapid emergence of dramatically superhuman intelligences.</p> <p><strong>Null Hypothesis :</strong> No creation of that SEED AGI to begin with.</p> <h1 id="papers-in-the-field-and-data-sources">Papers in the field and Data Sources</h1> <ol> <li> <p><a href="https://uol.primo.exlibrisgroup.com/discovery/fulldisplay?docid=cdi_askewsholts_vlebooks_9783642325601&amp;context=PC&amp;vid=353UOL_INST:353UOL_VU1&amp;lang=en&amp;search_scope=MyInst_and_CI&amp;adaptor=Primo%20Central&amp;tab=TAB1&amp;query=any,contains,singularity%20hypothesis&amp;sortby=rank" target="_blank" rel="noopener noreferrer">The Main Book</a> – UL Glucksman Library</p> </li> <li> <p><a href="https://link.springer.com/chapter/10.1007/978-3-642-32560-1_4" target="_blank" rel="noopener noreferrer">Research Question 1</a></p> </li> <li> <p><a href="https://lists.extropy.org/pipermail/extropy-chat/2011-January/063255.html" target="_blank" rel="noopener noreferrer">Research Question 2</a></p> </li> <li> <p><a href="https://otexts.com/fpp2/forecasting-regression.html" target="_blank" rel="noopener noreferrer">Forecasting model data</a> and <a href="https://commons.wikimedia.org/wiki/File:Estimations_of_Human_Brain_Emulation_Required_Performance.svg" target="_blank" rel="noopener noreferrer">Human level AGI forecast</a></p> </li> </ol> <p><strong>Relevant research:</strong></p> <ul> <li> <p>Baum, S. D., Goertzel, B., &amp; Goertzel, T. G. (2011). How long until human-level AI? results from an expert assessment. Technological Forecasting and Social Change, 78(1), 185–195.</p> </li> <li>Chalmers, D. (2010). The singularity: A philosophical analysis. Journal of Consciousness Studies, 17, 7–65.</li> <li>Goertzel, B. (2010). Toward a formal characterization of real-world general intelligence. Proceedings of AGI-10, Lugano.</li> <li>Hutter, M. (2005). Universal AI. Berlin: Springer.</li> <li>Sandberg, A. (2011, January 19). Limiting factors of intelligence explosion speeds. Extropy email</li> </ul> <p><strong>Data Sources:</strong></p> <p><a href="https://fred.stlouisfed.org/series/Y006RC1Q027SBEA" target="_blank" rel="noopener noreferrer">Private investment</a></p> <p><a href="https://ourworldindata.org/grapher/research-and-development-expenditure-of-gdp?tab=chart&amp;country=~OWID_WRL" target="_blank" rel="noopener noreferrer">Global GDP share on R&amp;D</a></p> <p><a href="https://ourworldindata.org/grapher/supercomputer-power-flops?time=earliest..2020" target="_blank" rel="noopener noreferrer">Super-Computer Power</a></p> <p><a href="https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-09-03" target="_blank" rel="noopener noreferrer">Hardware Data</a></p> <p><a href="https://ourworldindata.org/technological-progress" target="_blank" rel="noopener noreferrer">Transistors per Chip</a></p> <h1 id="model-and-variables-proposed">Model and Variables proposed</h1> <p>Quantum computing, carbon nanotube transistors, even spintronics, are enticing possibilities—but none are obvious replacements for the promise that Gordon Moore first saw in a simple integrated circuit. Hence I choose variables that are available to quantify my hypothesis.</p> <p><code class="language-plaintext highlighter-rouge">Creation of SEED AGI – Human Level AGI - Dependent variable</code></p> <ul> <li>Super computer power</li> <li>Investment ability in R&amp;D - Gathering of empirical information (experimentation, interacting with an environment)</li> <li>Software complexity</li> <li>Hardware advancement</li> </ul> <p><a href="https://commons.wikimedia.org/wiki/File:Estimations_of_Human_Brain_Emulation_Required_Performance.svg" target="_blank" rel="noopener noreferrer">Ln Human Level AGI</a> = \(Ln (Super computer power in FLOPS) + β1 x Investment in RnD + β2 x Tech-complexity ( Hardware + Software )\)</p> <p>I shall project SCP till 2040 maybe using - \([FLOPS] x 10^{\frac{(t‐2020)}{D}}\) 🡪 The equation is taken from <a href="https://www.fhi.ox.ac.uk/Reports/2008-3.pdf" target="_blank" rel="noopener noreferrer">Whole Brain Emulation</a>: A Roadmap page: 95</p> <ul> <li>D – Decade time in 1 order increment for Millions of Instructions Per Second. A measure of computing speed. ( difference in years – FLOP order 10^n and 10^n+1 )</li> <li>t - Year</li> </ul> <p><code class="language-plaintext highlighter-rouge">Probability of explosion - key limiting factor for how fast intelligence can amplify itself – Dependent variable </code></p> <ul> <li>AGI projection</li> <li>Economic growth rate</li> </ul> <h1 id="a-priori-expectation">A priori expectation</h1> <p>I think AGI creation is possible and I believe I can reject the NULL Hypothesis. Once AGI is formed the explosion is inevitable. Hence, the singularity hypothesis of history converging may not just be a faith driven irrelevant feud.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Mohan Ramesh. </div> </footer> <d-bibliography src="/assets/bibliography/"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>